Dataset: py150
Algorithm: ERM
Root dir: data1500-aug
Split scheme: official
Dataset kwargs: {}
Download: False
Frac: 1.0
Version: None
Unlabeled split: None
Unlabeled version: None
Use unlabeled y: False
Loader kwargs: {'num_workers': 4, 'pin_memory': True}
Unlabeled loader kwargs: {'num_workers': 8, 'pin_memory': True}
Train loader: standard
Uniform over groups: False
Distinct groups: None
N groups per batch: 2
Unlabeled n groups per batch: None
Batch size: 6
Unlabeled batch size: None
Eval loader: standard
Gradient accumulation steps: 1
Model: code-gpt-py
Model kwargs: {}
Noisystudent add dropout: None
Noisystudent dropout rate: None
Pretrained model path: None
Load featurizer only: False
Teacher model path: None
Transform: None
Additional train transform: None
Target resolution: None
Resize scale: None
Max token length: None
Randaugment n: 2
Loss function: lm_cross_entropy
Loss kwargs: {}
Groupby fields: ['repo']
Group dro step size: None
Coral penalty weight: 1.0
Dann penalty weight: None
Dann classifier lr: None
Dann featurizer lr: None
Dann discriminator lr: None
Afn penalty weight: None
Safn delta r: None
Hafn r: None
Use hafn: False
Irm lambda: 1.0
Irm penalty anneal iters: None
Self training lambda: None
Self training threshold: None
Pseudolabel t2: None
Soft pseudolabels: False
Algo log metric: multitask_accuracy
Process pseudolabels function: None
Val metric: acc
Val metric decreasing: False
N epochs: 3
Optimizer: AdamW
Lr: 8e-05
Weight decay: 0.0
Max grad norm: 1.0
Optimizer kwargs: {'eps': 1e-08}
Scheduler: linear_schedule_with_warmup
Scheduler kwargs: {'num_warmup_steps': 0}
Scheduler metric split: val
Scheduler metric name: None
Process outputs function: multiclass_logits_to_pred
Evaluate all splits: True
Eval splits: []
Eval only: False
Eval epoch: None
Device: cuda
Seed: 0
Log dir: /home/luka/logs/logs-add-print
Log every: 50
Save step: None
Save best: True
Save last: True
Save pred: True
No group logging: True
Progress bar: False
Resume: False
Use wandb: False
Wandb api key path: None
Wandb kwargs: {}
Use data parallel: False

Train data...
    n = 3000
Validation (OOD) data...
    n = 110
Test (OOD) data...
    n = 855
Validation (ID) data...
    n = 107
Test (ID) data...
    n = 428

Epoch [0]:

Train:
objective: 2.371
loss_all: 2.371
acc_all: 0.560

objective: 2.017
loss_all: 2.017
acc_all: 0.613

objective: 1.844
loss_all: 1.844
acc_all: 0.639

objective: 1.847
loss_all: 1.847
acc_all: 0.639

objective: 1.710
loss_all: 1.710
acc_all: 0.664

objective: 1.687
loss_all: 1.687
acc_all: 0.663

objective: 1.617
loss_all: 1.617
acc_all: 0.673

objective: 1.564
loss_all: 1.564
acc_all: 0.679

objective: 1.483
loss_all: 1.483
acc_all: 0.695

objective: 1.480
loss_all: 1.480
acc_all: 0.694

Epoch eval:
Acc (Class-Method): 0.624
Acc (Overall): 0.652
Acc (class): 0.618
Acc (method): 0.631
Acc (punctuation): 0.834
Acc (keyword): 0.633
Acc (builtin): 0.683
Acc (literal): 0.607
Acc (other_identifier): 0.509

Validation (OOD):
objective: 1.971
loss_all: 1.971
acc_all: 0.637

Epoch eval:
Acc (Class-Method): 0.611
Acc (Overall): 0.637
Acc (class): 0.616
Acc (method): 0.604
Acc (punctuation): 0.830
Acc (keyword): 0.634
Acc (builtin): 0.698
Acc (literal): 0.580
Acc (other_identifier): 0.479
Validation acc: 0.611
Epoch 0 has the best validation performance so far.


Epoch [1]:

Train:
objective: 1.149
loss_all: 1.149
acc_all: 0.745

objective: 1.126
loss_all: 1.126
acc_all: 0.748

objective: 1.128
loss_all: 1.128
acc_all: 0.752

objective: 1.096
loss_all: 1.096
acc_all: 0.752

objective: 1.080
loss_all: 1.080
acc_all: 0.757

objective: 1.045
loss_all: 1.045
acc_all: 0.764

objective: 0.984
loss_all: 0.984
acc_all: 0.773

objective: 1.025
loss_all: 1.025
acc_all: 0.768

objective: 0.987
loss_all: 0.987
acc_all: 0.776

objective: 0.969
loss_all: 0.969
acc_all: 0.774

Epoch eval:
Acc (Class-Method): 0.762
Acc (Overall): 0.761
Acc (class): 0.738
Acc (method): 0.790
Acc (punctuation): 0.907
Acc (keyword): 0.707
Acc (builtin): 0.783
Acc (literal): 0.735
Acc (other_identifier): 0.646

Validation (OOD):
objective: 2.164
loss_all: 2.164
acc_all: 0.630

Epoch eval:
Acc (Class-Method): 0.586
Acc (Overall): 0.629
Acc (class): 0.593
Acc (method): 0.577
Acc (punctuation): 0.829
Acc (keyword): 0.623
Acc (builtin): 0.702
Acc (literal): 0.565
Acc (other_identifier): 0.473
Validation acc: 0.586


Epoch [2]:

Train:
objective: 0.766
loss_all: 0.766
acc_all: 0.817

objective: 0.745
loss_all: 0.745
acc_all: 0.819

objective: 0.736
loss_all: 0.736
acc_all: 0.822

objective: 0.753
loss_all: 0.753
acc_all: 0.821

objective: 0.736
loss_all: 0.736
acc_all: 0.821

objective: 0.704
loss_all: 0.704
acc_all: 0.827

objective: 0.706
loss_all: 0.706
acc_all: 0.829

objective: 0.707
loss_all: 0.707
acc_all: 0.829

objective: 0.687
loss_all: 0.687
acc_all: 0.833

objective: 0.702
loss_all: 0.702
acc_all: 0.830

Epoch eval:
Acc (Class-Method): 0.833
Acc (Overall): 0.825
Acc (class): 0.801
Acc (method): 0.870
Acc (punctuation): 0.941
Acc (keyword): 0.753
Acc (builtin): 0.840
Acc (literal): 0.818
Acc (other_identifier): 0.733

Validation (OOD):
objective: 2.315
loss_all: 2.315
acc_all: 0.625

Epoch eval:
Acc (Class-Method): 0.580
Acc (Overall): 0.624
Acc (class): 0.583
Acc (method): 0.575
Acc (punctuation): 0.825
Acc (keyword): 0.619
Acc (builtin): 0.690
Acc (literal): 0.554
Acc (other_identifier): 0.469
Validation acc: 0.580

